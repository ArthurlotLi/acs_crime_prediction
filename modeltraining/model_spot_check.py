#
# model_spot_check.py
#
# Executes a spot check upon a particular dataset utilizing an
# assortment of algorithms from various algorithm families to
# demonstrate which provides the best performance. Utilizes 
# 10-fold cross validation for results. 

# Requires the following files:
#  - data_preprocessing.py

# Expects the iternum + solution to be present in the complete
# subdirectory, having been generated by autodataset.py.
#  - complete/complete_iterA#.csv

import argparse

from data_preprocessing import DataPreprocessing

# Algorithms to use
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn_lvq import GlvqModel
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.linear_model import LinearRegression, LogisticRegression, Lasso

# Pipeline process
from sklearn.model_selection import KFold, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from tensorflow.keras.wrappers.scikit_learn import KerasRegressor

class ModelSpotCheck:

  # Hyperparameters for spot-checked models. 
  nn_input_dim = -1 # Must be set beforehand. 
  nn_layer1 = 64
  nn_epochs = 200
  nn_batch_size = 32
  knn_n_neighbors = 5
  knn_metric = "manhattan"
  dt_criterion = "friedman_mse"
  rf_criterion = "friedman_mse"
  rf_n_estimators = 100
  rf_max_depth = None
  svm_C = 1.0
  svm_kernel="rbf"
  gb_criterion = "friedman_mse"
  gb_n_estimators = 100
  gb_subsample = 1.0
  gb_learning_rate = 0.1
  lda_solver = "svd"

  # Requires class variable nn_input_dim to be set beforehand. 
  def baseline_nn(self):
    model = Sequential()
    model.add(Dense(self.nn_layer1, input_dim=self.nn_input_dim, activation="relu"))
    model.add(Dense(1))
    model.compile(loss="mean_squared_error", optimizer="adam", metrics=["mse"])
    return model
  def baseline_knn(self):
    return KNeighborsRegressor(n_neighbors=self.knn_n_neighbors, metric=self.knn_metric)
  def baseline_dt(self):
    return DecisionTreeRegressor(criterion=self.dt_criterion)
  def baseline_rf(self):
    return RandomForestRegressor(criterion=self.rf_criterion, n_estimators=self.rf_n_estimators, max_depth=self.rf_max_depth)
  def baseline_svm(self):
    return SVR(C=self.svm_C, kernel=self.svm_kernel)
  def baseline_lvq(self):
    return GlvqModel()
  def baseline_gb(self):
    return GradientBoostingRegressor(criterion=self.gb_criterion, n_estimators=self.gb_n_estimators, subsample=self.gb_subsample, learning_rate = self.gb_learning_rate)
  def baseline_lda(self):
    return LinearDiscriminantAnalysis(solver=self.lda_solver)
  def baseline_logr(self):
    return LogisticRegression()
  def baseline_lr(self):
    return LinearRegression()
  def baseline_lassor(self):
    return Lasso()

  # Pipeline process methods.
  def model_pipeline(self, model, scaler = None):
    if scaler is None:
      scaler = StandardScaler()
    estimators = []
    estimators.append(("scaler",scaler))
    estimators.append(("model", model))
    return Pipeline(estimators)

  def model_evaluation(self, pipeline, X, y, scoring="neg_mean_absolute_error"):
    kfold = KFold(n_splits=10, shuffle=True)
    return cross_val_score(pipeline, X, y, cv=kfold, scoring=scoring)

  def model_print(self, results, model_name):
    print("[INFO] Baseline " + model_name + ": %.4f (%.4f)" % (-results.mean(),results.std()))

  def model_full(self, model, model_name, X, y, scoring="neg_mean_absolute_error", scaler=None):
    pipeline = self.model_pipeline(model, scaler)
    results = self.model_evaluation(pipeline, X, y, scoring)
    self.model_print(results, model_name)
    return results

  # Given all the algorithms we want to check, read and preprocess the
  # data before giving all of them a good college try. 
  def execute_spot_check(self, iternum, solution):
    print("[INFO] Executing model spot check for iteration " + str(iternum) + " - " + str(solution) + ".")

    # Obtain X and y using data preprocessing class. 
    data_preprocessing = DataPreprocessing()
    X, y = data_preprocessing.read_process_data(iternum, solution)

    if X is None or y is None:
      print("[ERROR] X and y are None - unable to proceed. Stopping...")
      return

    # Execute model spot check. 
    X_rows, X_cols = X.shape
    self.nn_input_dim = X_cols # For the NN input layer.

    print("[INFO] X and y read successfully. X dimensions: " + str(X_rows) + " x " + str(X_cols))
    print("[INFO] Executing spot check:")
    self.model_full(self.baseline_knn(), "knn", X, y)
    self.model_full(self.baseline_dt(), "dt", X, y)
    self.model_full(self.baseline_rf(), "rf", X, y)
    self.model_full(self.baseline_svm(), "svm", X, y)
    self.model_full(self.baseline_lvq(), "lvq", X, y)
    self.model_full(self.baseline_gb(), "gb", X, y)
    self.model_full(self.baseline_lda(), "lda", X, y)
    self.model_full(self.baseline_logr(), "logr", X, y)
    self.model_full(self.baseline_lr(), "lr", X, y)
    self.model_full(self.baseline_lassor(), "lassor", X, y)
    self.model_full(self.baseline_nn(), "nn", X, y)

if __name__ == "__main__":
  parser = argparse.ArgumentParser()
  parser.add_argument("iternum")
  parser.add_argument("solution")
  args = parser.parse_args()

  iternum = args.iternum
  solution = args.solution

  model_spot_check = ModelSpotCheck()
  model_spot_check.execute_spot_check(iternum, solution)